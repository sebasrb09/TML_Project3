{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be found here:\n",
    "https://github.com/arezae4/fair-logloss-classification\n",
    "\n",
    "based on this paper: https://arxiv.org/pdf/1903.03910.pdf\n",
    "\n",
    "in-process method that jointly optimizes a fairness transformation and linear feature-based parameters for an exponential family distribution that can be viewed as truncated logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC,abstractmethod\n",
    "from enum import Enum\n",
    "from math import isclose\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs, minimize\n",
    "import pdb\n",
    "__all__ = ['DP_fair_logloss_classifier','EODD_fair_logloss_classifier','EOPP_fair_logloss_classifier']\n",
    "\n",
    "def _log_logistic(X):\n",
    "    \"\"\" This function is used from scikit-learn source code. Source link below \"\"\"\n",
    "\n",
    "    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n",
    "    This implementation is numerically stable because it splits positive and\n",
    "    negative values::\n",
    "        -log(1 + exp(-x_i))     if x_i > 0\n",
    "        x_i - log(1 + exp(x_i)) if x_i <= 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like, shape (M, N)\n",
    "        Argument to the logistic function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: array, shape (M, N)\n",
    "        Log of the logistic function evaluated at every point in x\n",
    "    Notes\n",
    "    -----\n",
    "    Source code at:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py\n",
    "    -----\n",
    "\n",
    "    See the blog post describing this implementation:\n",
    "    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n",
    "    \"\"\"\n",
    "    if X.ndim > 1: raise Exception(\"Array of samples cannot be more than 1-D!\")\n",
    "    out = np.empty_like(X) # same dimensions and data types\n",
    "\n",
    "    idx = X>0\n",
    "    out[idx] = -np.log(1.0 + np.exp(-X[idx]))\n",
    "    out[~idx] = X[~idx] - np.log(1.0 + np.exp(X[~idx]))\n",
    "    return out\n",
    "\n",
    "def _dot_intercept(w, X):\n",
    "    \"\"\" This function is used from scikit-learn source code. Source link below \"\"\"\n",
    "\n",
    "    \"\"\"Computes y * np.dot(X, w).\n",
    "    It takes into consideration if the intercept should be fit or not.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n_features,)\n",
    "        Coefficient vector without the intercept weight (w[-1]) if the\n",
    "        intercept should be fit. Unchanged otherwise.\n",
    "    c : float\n",
    "        The intercept.\n",
    "    yz : float\n",
    "        y * np.dot(X, w).\n",
    "    \n",
    "    Notes\n",
    "\t-----\n",
    "\tSource code at:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py\n",
    "\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    if w.size == X.shape[1] + 1:\n",
    "        c = w[-1]\n",
    "        w = w[:-1]\n",
    "\n",
    "    z = np.dot(X, w) + c\n",
    "    return z\n",
    "\n",
    "def sum_truncated_loss_grad(theta, X, Y, idx1, idx0, _lambda ):\n",
    "    z = _dot_intercept(theta, X)\n",
    "    p = np.exp(_log_logistic(z))\n",
    "    loss , grad = 0, 0\n",
    "    if _lambda == 0:\n",
    "        loss, grad = sum_logistic_loss_grad(theta,X,Y, np.logical_or(idx1 , idx0))\n",
    "        cond_g1, cond_g0 = idx1, idx0\n",
    "    else:\n",
    "        p1 = np.mean(idx1)   # empirical probability of each group\n",
    "        p0 = np.mean(idx0)\n",
    "        if _lambda > 0:\n",
    "            cond_g1 = np.logical_and( idx1 , p > p1 / _lambda)\n",
    "            cond_g0 = np.logical_and( idx0 , p < (1 - p0 / _lambda))\n",
    "\n",
    "            grad = np.sum(X[np.logical_and(cond_g1, 1 - Y)] ,0) + np.sum(-X[np.logical_and(cond_g0, Y)], 0)\n",
    "            loss = (np.log(_lambda /p1)) * np.sum(cond_g1,0) + np.sum(_dot_intercept(theta,X[cond_g1]) * (1-Y[cond_g1]),0) \\\n",
    "                   + (np.log(_lambda/p0)) * np.sum(cond_g0,0) + np.sum(-_dot_intercept(theta,X[cond_g0]) * Y[cond_g0],0) \n",
    "        else:\n",
    "            cond_g1 = np.logical_and( idx1 , p < (1 + p1 / _lambda))\n",
    "            cond_g0 = np.logical_and( idx0 , p > - p0 / _lambda)\n",
    "\n",
    "            grad = np.sum(X[np.logical_and(cond_g0, 1 - Y)],0) + np.sum(-X[np.logical_and(cond_g1, Y)], 0)\n",
    "            loss = (np.log(-_lambda/p1)) * np.sum(cond_g1,0) + np.sum(-_dot_intercept(theta,X[cond_g1]) * Y[cond_g1],0) \\\n",
    "                   + (np.log(-_lambda/p0)) * np.sum(cond_g0,0) + np.sum(_dot_intercept(theta,X[cond_g0]) * (1-Y[cond_g0]),0) \n",
    "    return loss, grad, np.logical_or(cond_g1, cond_g0)             \n",
    "\n",
    "def sum_logistic_loss_grad(theta, X, Y, idx):\n",
    "    X, Y = X[idx,:], Y[idx]\n",
    "    z = _dot_intercept(theta, X)\n",
    "    p = np.exp(_log_logistic(z))\n",
    "    grad = np.dot(p.T, X) - np.sum(X[Y == 1,:],0)\n",
    "\n",
    "    logZ = z + np.log(1 + np.exp(-z))\n",
    "    loss = np.sum(logZ,0) - np.sum(z * Y,0)\n",
    "    \n",
    "    return loss, grad \n",
    "\n",
    "def fairify(a,b):\n",
    "    avgA, avgB = np.mean(a), np.mean(b)\n",
    "    if a.size == 0:\n",
    "        return np.Inf \n",
    "    elif b.size == 0:\n",
    "        return np.NINF\n",
    "    _lambda = 0\n",
    "    if isclose(avgA,avgB):\n",
    "        return _lambda  # already fair\n",
    "    flipped = False\n",
    "    if avgA < avgB:\n",
    "        b, a = a, b\n",
    "        avgA, avgB = avgB, avgA\n",
    "        flipped = True\n",
    "    diff = avgA - avgB\n",
    "    if diff < 0:\n",
    "        raise ValueError('_lambda is not supposed to be negative')\n",
    "    \n",
    "    a = - np.sort(-a)     # sort descending\n",
    "    b.sort()                 # sort ascending\n",
    "\n",
    "    idxA, idxB = 0, 0   # current index\n",
    "    thrA, thrB = 1.0, 0.0   # current probability threshold\n",
    "    gainA, gainB = 0, 0     # average gain in each group\n",
    "\n",
    "    while True:\n",
    "        if idxA < len(a):\n",
    "            cd_thrA = a[idxA]\n",
    "        else:\n",
    "            cd_thrA = 0.0\n",
    "        cd_thrB_ifA = 1 - (cd_thrA * len(b) / len(a))\n",
    "\n",
    "        if idxB < len(b):\n",
    "            cd_thrB = b[idxB]\n",
    "        else:\n",
    "            cd_thrB = 1.0\n",
    "        \n",
    "        if cd_thrB_ifA <= cd_thrB:\n",
    "            next_thrA = cd_thrA\n",
    "            next_thrB = cd_thrB_ifA\n",
    "        else:\n",
    "            next_thrA = (1 - cd_thrB) * len(a) / len(b)\n",
    "            next_thrB = cd_thrB\n",
    "\n",
    "        next_gainA = gainA + idxA * (thrA - next_thrA) / len(a)\n",
    "        next_gainB = gainB + idxB * (next_thrB - thrB) / len(b)\n",
    "\n",
    "        if isclose(next_gainA + next_gainB , diff):\n",
    "            thrA = next_thrA\n",
    "            _lambda = len(a) / thrA\n",
    "            break\n",
    "        elif next_gainA + next_gainB < diff:\n",
    "            thrA = next_thrA\n",
    "            thrB = next_thrB\n",
    "            if cd_thrB_ifA <= cd_thrB:\n",
    "                idxA += 1\n",
    "            else:\n",
    "                idxB += 1\n",
    "            gainA = next_gainA\n",
    "            gainB = next_gainB\n",
    "        else:\n",
    "            gain_needed = diff - gainA - gainB\n",
    "            _lambda = (idxA + idxB) / (idxA * thrA / len(a) + idxB * (1 - thrB) / len(b) - gain_needed)\n",
    "            break\n",
    "    thrA = len(a) / _lambda\n",
    "    thrB = 1 - len(b) / _lambda\n",
    "    if flipped:\n",
    "        _lambda = -_lambda\n",
    "    avgA = np.mean(np.minimum(a, thrA))\n",
    "    avgB = np.mean(np.maximum(b, thrB))\n",
    "\n",
    "    if not isclose(avgA , avgB):\n",
    "        raise ValueError('Averages not equalized %.3f vs %.3f, diff was %.3f' % (avgA, avgB, diff) )\n",
    "    return _lambda \n",
    "\n",
    "\n",
    "class fair_logloss_classifier:\n",
    "    def __init__(self, tol=1e-6, verbose=True, max_iter=10000, C = .001, random_initialization=False):\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.random_start = random_initialization\n",
    "        self.theta = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_grad(self, theta,  X, Y):\n",
    "        pass        \n",
    "    @abstractmethod\n",
    "    def _group_protected_attribute(self, Y, A):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,Y,A):\n",
    "        n = np.size(Y)\n",
    "        X = np.hstack((X,np.ones((n,1))))\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        self._group_protected_attribute(Y,A)\n",
    "\n",
    "        if self.random_start:\n",
    "            theta = np.random.random_sample(m) - .5 \n",
    "        else:\n",
    "            theta = np.zeros((m,))\n",
    "        #f = lambda w : self.compute_loss_grad(w,X, Y)[0]\n",
    "        #grad = lambda w : self.compute_loss_grad(w,X, Y)[1] \n",
    "        def callback(w):\n",
    "            f, g = self.compute_loss_grad(w,X,Y)\n",
    "            print(\"fun_value {:.4f} \\t gnorm {:.4f}\".format(f,np.linalg.norm(g)))\n",
    "        #res = fmin_bfgs(f,theta, grad, gtol=self.tol, maxiter=self.max_iter,full_output=False,disp=True, retall=True, callback = callback)\n",
    "        #res = minimize(self.compute_loss_grad, theta,args=(X, Y), method='L-BFGS-B',jac=True, tol=self.tol, options={'maxiter':self.max_iter, 'disp':False}, callback=callback)\n",
    "        res = minimize(self.compute_loss_grad, theta,args=(X, Y), method='L-BFGS-B',jac=True, tol=self.tol, options={'maxiter':self.max_iter, 'disp':self.verbose})\n",
    "         \n",
    "        self.theta = res.x\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba_given_y(self,X,Y,A):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def predict_proba(self,X,A):\n",
    "        pass\n",
    "\n",
    "    def _fixed_point_proba(self,X,A):\n",
    "        phat1, pcheck1 = self.predict_proba_given_y(X,np.ones_like(A),A)\n",
    "        phat0, pcheck0 = self.predict_proba_given_y(X,np.zeros_like(A),A)\n",
    "        prob = (phat1 * pcheck0 + phat0 * (1 - pcheck1)) / (1 - pcheck1 + pcheck0)\n",
    "        return prob\n",
    "         \n",
    "    def _predict_proba_given_y(self, X, Y, _lambda, grp1, grp2, p1, p2):\n",
    "        \"\"\"\n",
    "            Computes \\hat{P}(\\hat{Y}|X,A,Y) for two groups\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples,n_features + 1)\n",
    "            Feature vector\n",
    "        Y : ndarray, shape (n_samples,)\n",
    "            Array of labels.\n",
    "        _lambda : float \n",
    "            Fairness parameter for the given two groups\n",
    "        grp1, grp2 : ndarray, shape(n_samples,)\n",
    "            indices for group members\n",
    "        p1, p2, : float\n",
    "            empirical probability of each group from training data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        phat: array, shape (n_samples,) \n",
    "            Prediction probability\n",
    "        pcheck : array, shape (n_samples,)\n",
    "            Adversarial estimation of the empirical distribution\n",
    "        \"\"\"\n",
    "        phat = np.exp(_log_logistic(_dot_intercept(self.theta,X)))\n",
    "        pcheck = np.copy(phat)\n",
    "        \n",
    "        if _lambda > 0:\n",
    "            cond = (p1 / _lambda) < phat\n",
    "            phat[grp1] = np.minimum( phat[grp1] , p1 /_lambda)\n",
    "            pcheck[grp1] = np.where( cond[grp1], np.ones_like(pcheck[grp1]), phat[grp1] * (1 + (_lambda / p1) * (1 - phat[grp1])))\n",
    "            \n",
    "            cond = (1 - (p2 / _lambda)) > phat\n",
    "            phat[grp2] = np.maximum( phat[grp2] , 1 - p2 /_lambda)\n",
    "            pcheck[grp2] = np.where( cond[grp2], np.zeros_like(cond[grp2]), phat[grp2] * (1 - (_lambda / p2) * (1 - phat[grp2])))\n",
    "        elif _lambda < 0:\n",
    "            cond = (1 + (p1 / _lambda)) > phat\n",
    "            phat[grp1] = np.maximum( phat[grp1] , 1 + p1 /_lambda)\n",
    "            pcheck[grp1] = np.where( cond[grp1], np.zeros_like(cond[grp1]), phat[grp1] * (1 - (_lambda / p1) * (1 - phat[grp1])))\n",
    "            \n",
    "            cond = (- p2 / _lambda) < phat\n",
    "            phat[grp2] = np.minimum( phat[grp2] , - p2 /_lambda)\n",
    "            pcheck[grp2] = np.where( cond[grp2], np.ones_like(cond[grp2]), phat[grp2] * (1 + (_lambda / p2) * (1 - phat[grp2])))\n",
    "        return phat, pcheck\n",
    "\n",
    "    def predict(self,X,A):\n",
    "        return np.round(self.predict_proba(X,A))\n",
    "\n",
    "    @abstractmethod\n",
    "    def fairness_violation(self,X,Y,A):\n",
    "        pass\n",
    "    def score(self,X,Y,A):\n",
    "        return 1 - np.mean(abs(self.predict(X,A) - Y))\n",
    "    def expected_error(self,X,Y,A):\n",
    "        proba = self.predict_proba(X,A)\n",
    "        return np.mean(np.where(Y == 1 , 1 - proba, proba))\n",
    "\n",
    "class DP_fair_logloss_classifier(fair_logloss_classifier):\n",
    "    def __init__(self, tol=1e-6, verbose=True, max_iter=10000, C = .1, random_initialization=False):\n",
    "        super().__init__(tol = tol, verbose = verbose, max_iter = max_iter, C = C, random_initialization=random_initialization)\n",
    "\n",
    "    def _group_protected_attribute(self, tr_Y, tr_A):\n",
    "        self.grp1 = tr_A == 1\n",
    "        self.grp2 = tr_A == 0\n",
    "\n",
    "    def compute_loss_grad(self, theta, X, Y):\n",
    "        p = np.exp(_log_logistic(_dot_intercept(theta,X)))\n",
    "        idx1 = self.grp1 # A == 1\n",
    "        idx0 = self.grp2 # A == 0\n",
    "        n = X.shape[0]\n",
    "        self._lambda = fairify(p[idx1], p[idx0]) / n\n",
    "        loss, grad, trunc_idx = sum_truncated_loss_grad(theta, X, Y, idx1, idx0, self._lambda)\n",
    "        loss_ow, grad_ow = sum_logistic_loss_grad(theta,X,Y, np.logical_not(trunc_idx))\n",
    "\n",
    "        loss += loss_ow\n",
    "        grad += grad_ow\n",
    "        \n",
    "        loss = loss / n + .5 * self.C * np.dot(theta, theta)\n",
    "        grad = grad / n + self.C * theta\n",
    "        return loss, grad\n",
    "    \n",
    "    def predict_proba_given_y(self,X,Y,A):\n",
    "        grp1 = A == 1\n",
    "        grp2 = A == 0\n",
    "        p1, p2 = np.mean(self.grp1) , np.mean(self.grp2) # group empirical probability based on training data\n",
    "        \n",
    "        return self._predict_proba_given_y(X,Y, self._lambda, grp1, grp2, p1, p2)\n",
    "                   \n",
    "    def predict_proba(self,X,A):\n",
    "        return self.predict_proba_given_y(X,np.empty_like(A),A)[0]\n",
    "\n",
    "    def fairness_violation(self,X,Y,A):\n",
    "        proba = self.predict_proba(X,A)\n",
    "        return abs(np.mean(proba[A == 1]) - np.mean(proba[A == 0]))\n",
    "\n",
    "\n",
    "class EOPP_fair_logloss_classifier(fair_logloss_classifier):\n",
    "    def __init__(self, tol=1e-6, verbose=True, max_iter=10000, C = .1, random_initialization=False):\n",
    "        super().__init__(tol = tol, verbose = verbose, max_iter = max_iter, C = C, random_initialization= random_initialization)\n",
    "\n",
    "    def _group_protected_attribute(self, tr_Y, tr_A):\n",
    "        self.grp1 = np.logical_and( tr_A == 1, tr_Y == 1)\n",
    "        self.grp2 = np.logical_and( tr_A == 0, tr_Y == 1)\n",
    "\n",
    "    def compute_loss_grad(self, theta, X, Y):\n",
    "        p = np.exp(_log_logistic(_dot_intercept(theta,X)))\n",
    "        idx1 = self.grp1 # np.logical_and(A == 1, Y == 1)\n",
    "        idx0 = self.grp2 # np.logical_and(A == 0, Y == 1)\n",
    "        n = X.shape[0]\n",
    "        self._lambda = fairify(p[idx1], p[idx0]) / n \n",
    "        loss, grad, trunc_idx  = sum_truncated_loss_grad(theta, X, Y, idx1, idx0, self._lambda)\n",
    "        loss_ow, grad_ow = sum_logistic_loss_grad(theta,X,Y, np.logical_not(trunc_idx))\n",
    "\n",
    "        loss += loss_ow\n",
    "        grad += grad_ow\n",
    "        loss = loss / n + .5 * self.C * np.dot(theta, theta)\n",
    "        grad = grad /n + self.C * theta\n",
    "        return loss, grad\n",
    "\n",
    "    def predict_proba_given_y(self,X,Y,A):\n",
    "        grp1 = np.logical_and(A == 1, Y == 1)\n",
    "        grp2 = np.logical_and(A == 0, Y == 1)\n",
    "        p1, p2 = np.mean(self.grp1) , np.mean(self.grp2)\n",
    "        return self._predict_proba_given_y(X,Y, self._lambda, grp1, grp2, p1, p2)\n",
    "        \n",
    "            \n",
    "    def predict_proba(self,X,A):\n",
    "        return self._fixed_point_proba(X,A)\n",
    "\n",
    "    def fairness_violation(self,X,Y,A):\n",
    "        proba = self.predict_proba(X,A)\n",
    "        return abs(np.mean(proba[np.logical_and(Y == 1, A == 1)]) - np.mean(proba[np.logical_and(Y == 1, A == 0)]))  \n",
    "    \n",
    "\n",
    "class EODD_fair_logloss_classifier(fair_logloss_classifier):\n",
    "    def __init__(self, tol=1e-6, verbose=True, max_iter=10000, C = .1, random_initialization=False):\n",
    "        super().__init__(tol = tol, verbose = verbose, max_iter = max_iter, C = C, random_initialization=random_initialization)\n",
    "\n",
    "    def _group_protected_attribute(self, tr_Y, tr_A):\n",
    "        self.grp1 = np.logical_and( tr_A == 1, tr_Y == 1)\n",
    "        self.grp2 = np.logical_and( tr_A == 0, tr_Y == 1)\n",
    "        self.grp3 = np.logical_and( tr_A == 1, tr_Y == 0)\n",
    "        self.grp4 = np.logical_and( tr_A == 0, tr_Y == 0)\n",
    "         \n",
    "    def compute_loss_grad(self, theta,X, Y):\n",
    "        p = np.exp(_log_logistic(_dot_intercept(theta,X)))\n",
    "        n = X.shape[0]\n",
    "        idx11 = self.grp1 # np.logical_and(A == 1, Y == 1)\n",
    "        idx01 = self.grp2 # np.logical_and(A == 0, Y == 1)\n",
    "        self._lambda1 = fairify(p[idx11], p[idx01]) / n \n",
    "\n",
    "        loss1, grad1, trunc_idx1 = sum_truncated_loss_grad(theta, X, Y, idx11, idx01, self._lambda1)\n",
    "\n",
    "        idx10 = self.grp3 # np.logical_and(A == 1, Y == 0)\n",
    "        idx00 = self.grp4 # np.logical_and(A == 0, Y == 0)\n",
    "        self._lambda0 = fairify(p[idx10], p[idx00]) / n \n",
    "\n",
    "        loss0, grad0, trunc_idx2 = sum_truncated_loss_grad(theta, X, Y, idx10, idx00, self._lambda0)\n",
    "        \n",
    "        loss_ow, grad_ow = sum_logistic_loss_grad(theta, X, Y, np.logical_not(np.logical_or(trunc_idx1, trunc_idx2)))\n",
    "\n",
    "        loss = loss1 + loss0 + loss_ow\n",
    "        grad = grad1 + grad0 + grad_ow\n",
    "        loss = loss / n + .5 * self.C * np.dot(theta, theta)\n",
    "        grad = grad /n + self.C * theta\n",
    "        #pdb.set_trace()\n",
    "        return loss, grad    \n",
    "    \n",
    "    def predict_proba_given_y(self,X,Y,A):\n",
    "        grp1 = np.logical_and(A == 1, Y == 1)\n",
    "        grp2 = np.logical_and(A == 0, Y == 1)\n",
    "        p1, p2 = np.mean(self.grp1) , np.mean(self.grp2)\n",
    "        phat, pcheck = self._predict_proba_given_y(X, Y, self._lambda1, grp1, grp2, p1, p2)\n",
    "        \n",
    "        grp3 = np.logical_and(A == 1, Y == 0)\n",
    "        grp4 = np.logical_and(A == 0, Y == 0)\n",
    "        p3, p4 = np.mean(self.grp3) , np.mean(self.grp4)\n",
    "        phat_, pcheck_ = self._predict_proba_given_y(X, Y, self._lambda0, grp3, grp4, p3, p4)\n",
    "        idx = np.logical_or(grp3, grp4)\n",
    "        phat[idx], pcheck[idx] = phat_[idx], pcheck_[idx]\n",
    "\n",
    "        return phat, pcheck\n",
    "        \n",
    "\n",
    "    def predict_proba(self,X,A):\n",
    "        return self._fixed_point_proba(X,A)\n",
    "        \n",
    "    def fairness_violation(self,X,Y,A):\n",
    "        proba = self.predict_proba(X,A)\n",
    "        return  abs(np.mean(proba[np.logical_and(Y == 1, A == 1)]) - np.mean(proba[np.logical_and(Y == 1, A == 0)]))  \\\n",
    "            +   abs(np.mean(proba[np.logical_and(Y == 0, A == 1)]) - np.mean(proba[np.logical_and(Y == 0, A == 0)]))\n",
    "   \n",
    "     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_IBM_adult():\n",
    "    dataA = pd.read_csv('IBM_adult_A.csv',sep='\\t',index_col = 0,header=None)#,usecols=range(1,2))\n",
    "    dataY = pd.read_csv('IBM_adult_Y.csv',sep='\\t',index_col = 0,header=None)#,usecols=range(0,2))\n",
    "    dataX = pd.read_csv('IBM_adult_X.csv',sep='\\t',index_col = 0)\n",
    "    perm = np.genfromtxt('adult_perm.csv', delimiter=',')\n",
    "    return dataA.iloc[:,0],dataY.iloc[:,0],dataX,perm \n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 1 ----------------------------------\n",
      "Train - predict_err : 0.154 \t expected_err : 0.284 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.159 \t expected_err : 0.286 \t fair_violation : 0.006 \n",
      "accuracy: 0.8409375691014963\n",
      "C = 0.9454190314734282\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8439596078720425\n",
      "C = 0.9389327043561583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 2 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.282 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.154 \t expected_err : 0.281 \t fair_violation : 0.002 \n",
      "accuracy: 0.8463919805410186\n",
      "C = 0.9432298960713495\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.850372226726616\n",
      "C = 0.9394928871526498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 3 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.280 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.155 \t expected_err : 0.281 \t fair_violation : 0.002 \n",
      "accuracy: 0.844696690499005\n",
      "C = 0.9433109751603155\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8463182722783223\n",
      "C = 0.9362939485516326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 4 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.285 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.156 \t expected_err : 0.285 \t fair_violation : 0.008 \n",
      "accuracy: 0.8435173582958649\n",
      "C = 0.9439448662195032\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8470553549052848\n",
      "C = 0.9392865040171003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 5 ----------------------------------\n",
      "Train - predict_err : 0.153 \t expected_err : 0.280 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.159 \t expected_err : 0.282 \t fair_violation : 0.003 \n",
      "accuracy: 0.8407164443134075\n",
      "C = 0.9417336183386158\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8427065674062062\n",
      "C = 0.9344070170266087\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 6 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.291 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.153 \t expected_err : 0.289 \t fair_violation : 0.006 \n",
      "accuracy: 0.8471290631679811\n",
      "C = 0.9451168275963735\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8468342301171962\n",
      "C = 0.9396181911992334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 7 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.281 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.154 \t expected_err : 0.281 \t fair_violation : 0.001 \n",
      "accuracy: 0.8464656888037149\n",
      "C = 0.9420653055207489\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8500036854131348\n",
      "C = 0.9375396181911992\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 8 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.280 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.154 \t expected_err : 0.277 \t fair_violation : 0.008 \n",
      "accuracy: 0.8463919805410186\n",
      "C = 0.9475197169602713\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8492666027861723\n",
      "C = 0.9418441807326601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 9 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.282 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.155 \t expected_err : 0.282 \t fair_violation : 0.003 \n",
      "accuracy: 0.8449178152870936\n",
      "C = 0.9407385567922164\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8479398540576398\n",
      "C = 0.9366551190388442\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 10 ----------------------------------\n",
      "Train - predict_err : 0.153 \t expected_err : 0.278 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.161 \t expected_err : 0.282 \t fair_violation : 0.004 \n",
      "accuracy: 0.8385051964325201\n",
      "C = 0.9472838505196433\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8445492739736125\n",
      "C = 0.9402668239109604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 11 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.287 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.152 \t expected_err : 0.284 \t fair_violation : 0.007 \n",
      "accuracy: 0.8478661457949436\n",
      "C = 0.9464067221935579\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8504459349893123\n",
      "C = 0.9395518537628068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 12 ----------------------------------\n",
      "Train - predict_err : 0.153 \t expected_err : 0.283 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.155 \t expected_err : 0.284 \t fair_violation : 0.002 \n",
      "accuracy: 0.8447703987617012\n",
      "C = 0.9463182722783224\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8483083953711211\n",
      "C = 0.9400162158177932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 13 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.280 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.154 \t expected_err : 0.282 \t fair_violation : 0.006 \n",
      "accuracy: 0.8455811896513599\n",
      "C = 0.9440112036559298\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8469816466425887\n",
      "C = 0.9387115795680696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 14 ----------------------------------\n",
      "Train - predict_err : 0.153 \t expected_err : 0.285 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.157 \t expected_err : 0.288 \t fair_violation : 0.004 \n",
      "accuracy: 0.8427065674062062\n",
      "C = 0.9442618117490971\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8457286061767524\n",
      "C = 0.9384978256062505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 15 ----------------------------------\n",
      "Train - predict_err : 0.155 \t expected_err : 0.285 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.154 \t expected_err : 0.286 \t fair_violation : 0.004 \n",
      "accuracy: 0.8455074813886637\n",
      "C = 0.9454558856047763\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8475713127441586\n",
      "C = 0.938763175351957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 16 ----------------------------------\n",
      "Train - predict_err : 0.157 \t expected_err : 0.276 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.152 \t expected_err : 0.274 \t fair_violation : 0.008 \n",
      "accuracy: 0.8484558118965136\n",
      "C = 0.9448957028082848\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8501511019385273\n",
      "C = 0.9401341490381072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 17 ----------------------------------\n",
      "Train - predict_err : 0.154 \t expected_err : 0.284 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.157 \t expected_err : 0.285 \t fair_violation : 0.001 \n",
      "accuracy: 0.8430751087196875\n",
      "C = 0.9445713864524213\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8460971474902337\n",
      "C = 0.9390137834451242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 18 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.286 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.151 \t expected_err : 0.284 \t fair_violation : 0.003 \n",
      "accuracy: 0.8488980614726911\n",
      "C = 0.9463993513672883\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8506670597774011\n",
      "C = 0.942338026092725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 19 ----------------------------------\n",
      "Train - predict_err : 0.156 \t expected_err : 0.278 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.155 \t expected_err : 0.278 \t fair_violation : 0.000 \n",
      "accuracy: 0.8452863566005749\n",
      "C = 0.9456917520454042\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.8487506449472986\n",
      "C = 0.9402520822584212\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beate\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Random Split 20 ----------------------------------\n",
      "Train - predict_err : 0.152 \t expected_err : 0.280 \t fair_violation : 0.000 \n",
      "Test  - predict_err : 0.162 \t expected_err : 0.285 \t fair_violation : 0.004 \n",
      "accuracy: 0.8375469890174688\n",
      "C = 0.9429350630205646\n",
      "for non-fair logistic regression:\n",
      "accuracy: 0.842927692194295\n",
      "C = 0.9354020785730081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from scipy.io import arff\n",
    "# from prepare_data import prepare_compas,prepare_IBM_adult, prepare_law\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# from fair_logloss import DP_fair_logloss_classifier, EOPP_fair_logloss_classifier, EODD_fair_logloss_classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def compute_error(Yhat,proba,Y):\n",
    "    err = 1 - np.sum(Yhat == Y) / Y.shape[0] \n",
    "    exp_zeroone = np.mean(np.where(Y == 1 , 1 - proba, proba))\n",
    "    return err, exp_zeroone\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataA,dataY,dataX,perm = prepare_IBM_adult()\n",
    "    dataset = \"adult\"\n",
    "    C = .005\n",
    "    criteria = 'dp'#sys.argv[2]\n",
    "    if criteria == 'dp':\n",
    "        h = DP_fair_logloss_classifier(C=C, random_initialization=True, verbose=False)\n",
    "    elif criteria == 'eqopp':\n",
    "        h = EOPP_fair_logloss_classifier(C=C, random_initialization=True, verbose=False)\n",
    "    elif criteria == 'eqodd':\n",
    "        h = EODD_fair_logloss_classifier(C=C, random_initialization=True, verbose=False)    \n",
    "    else:\n",
    "        raise ValueError('Invalid second arg')\n",
    "    filename_tr = \"fairll_{}_{:.3f}_{}_tr.csv\".format(dataset,C,criteria)\n",
    "    filename_ts = \"fairll_{}_{:.3f}_{}_ts.csv\".format(dataset,C,criteria)\n",
    "    \n",
    "    # outfile_tr = open(filename_tr,\"w\")\n",
    "    # outfile_ts = open(filename_ts,\"w\")\n",
    "\n",
    "    for r in range(20):\n",
    "        order = perm[r,:]\n",
    "        tr_sz = int(np.floor(.7 * dataX.shape[0]))\n",
    "        tr_idx = order[:tr_sz]\n",
    "        ts_idx = order[tr_sz:]\n",
    "        tr_X = dataX.reindex(tr_idx)\n",
    "        ts_X = dataX.reindex(ts_idx)\n",
    "        \n",
    "        tr_A = dataA.reindex(tr_X.index)\n",
    "        ts_A = dataA.reindex(ts_X.index)\n",
    "        tr_Y = dataY.reindex(tr_X.index)\n",
    "        ts_Y = dataY.reindex(ts_X.index)\n",
    "        \n",
    "        # Comment out to not include A in features\n",
    "        tr_X = pd.concat([tr_X, tr_A], axis=1) \t\n",
    "        ts_X = pd.concat([ts_X, ts_A], axis=1)\n",
    "        # ---------\n",
    "\n",
    "        for c in list(tr_X.columns):\n",
    "            if tr_X[c].min() < 0 or tr_X[c].max() > 1:\n",
    "                mu = tr_X[c].mean()\n",
    "                s = tr_X[c].std(ddof=0)\n",
    "                tr_X.loc[:,c] = (tr_X[c] - mu) / s\n",
    "                ts_X.loc[:,c] = (ts_X[c] - mu) / s\n",
    "        \n",
    "        h.fit(tr_X.values,tr_Y.values,tr_A.values)\n",
    "        exp_zo_tr = h.expected_error(tr_X.values, tr_Y.values, tr_A.values)\n",
    "        exp_zo_ts = h.expected_error(ts_X.values, ts_Y.values, ts_A.values)\n",
    "        err_tr = 1 - h.score(tr_X.values, tr_Y.values, tr_A.values)\n",
    "        err_ts = 1 - h.score(ts_X.values, ts_Y.values, ts_A.values)\n",
    "        violation_tr = h.fairness_violation(tr_X.values, tr_Y.values, tr_A.values)\n",
    "        violation_ts = h.fairness_violation(ts_X.values, ts_Y.values, ts_A.values)\n",
    "        prediction = h.predict(ts_X.values, ts_A.values)\n",
    "\n",
    "        # calculate consistency measure \n",
    "        # we use sklearn to find the nearest neighbors\n",
    "        neigh = NearestNeighbors(n_neighbors=11) # the first neighbour is always the data itself, therefore take 11 neighbours\n",
    "        nbrs = neigh.fit(ts_X.values)\n",
    "        distances, indices = nbrs.kneighbors(ts_X.values)\n",
    "        # calculate C\n",
    "        sum = 0\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(11):\n",
    "                sum += np.abs(int(prediction[i])-int(prediction[indices[i][j]]))\n",
    "        result = 1 - sum*(1/(10*len(indices)))\n",
    "        \n",
    "\n",
    "        # train logistic regression \n",
    "        clf = LogisticRegression(random_state=0).fit(tr_X.values, tr_Y.values)\n",
    "        logistic_prediction = clf.predict(ts_X.values)\n",
    "        # calculate C\n",
    "        sum = 0\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(11):\n",
    "                sum += np.abs(int(logistic_prediction[i])-int(logistic_prediction[indices[i][j]]))\n",
    "        result_logistic_regression = 1 - sum*(1/(10*len(indices)))\n",
    "        \n",
    "        print(\"---------------------------- Random Split %d ----------------------------------\" % (r + 1))\n",
    "        print(\"Train - predict_err : {:.3f} \\t expected_err : {:.3f} \\t fair_violation : {:.3f} \".format(err_tr, exp_zo_tr,violation_tr))\n",
    "        print(\"Test  - predict_err : {:.3f} \\t expected_err : {:.3f} \\t fair_violation : {:.3f} \".format(err_ts, exp_zo_ts,violation_ts))\n",
    "        print(\"accuracy:\", accuracy_score(ts_Y.values, prediction))\n",
    "        print(\"C =\",result)\n",
    "        print(\"for non-fair logistic regression:\")\n",
    "        print(\"accuracy:\", accuracy_score(ts_Y.values, logistic_prediction))\n",
    "        print(\"C =\",result_logistic_regression)\n",
    "        print(\"\")\n",
    "\n",
    "    #     outfile_ts.write(\"{:.4f},{:.4f},{:.4f}\\n\".format(exp_zo_ts,err_ts, violation_ts))\n",
    "    #     outfile_tr.write(\"{:.4f},{:.4f},{:.4f}\\n\".format(exp_zo_tr,err_tr, violation_tr))\n",
    "        \n",
    "    # outfile_tr.close()\n",
    "    # outfile_ts.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C increases by using the fair logistic regression version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48e4fac23f847e31e50a977181370ac78afc4be9236ebe86c805f672a975b811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
